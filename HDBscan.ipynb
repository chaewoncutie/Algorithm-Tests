{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORTR5x93iebPsBf95x33HA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaewoncutie/Algorithm-Tests/blob/main/HDBscan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Packages\n"
      ],
      "metadata": {
        "id": "ws2Io9drau4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install -U scikit-learn nltk pandas matplotlib seaborn scipy wordcloud ipywidgets umap-learn hdbscan\n"
      ],
      "metadata": {
        "id": "lqMEf2Bp6Uu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "AgDVe4U6ay9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, precision_score, accuracy_score\n",
        "import hdbscan\n",
        "from wordcloud import WordCloud\n",
        "import numpy as np\n",
        "import umap\n",
        "from google.colab import files\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "TcJeyV-u6VO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "ap37EXL06e4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning\n"
      ],
      "metadata": {
        "id": "uaMl6EqubJex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the JSON file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Read the JSON file\n",
        "file_name = next(iter(uploaded))\n",
        "df = pd.read_json(file_name, lines=True)\n",
        "\n",
        "# Display the original dataset information\n",
        "print(\"Original Dataset Shape:\", df.shape)\n",
        "print(\"Original Dataset Preview:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "GCZOaYEZ6VnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove whitespace from column names\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Count and identify null values\n",
        "print(\"\\nNull Values per Column:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "PZDggHzo6p8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with null values in 'headline' or 'short_description'\n",
        "df = df.dropna(subset=['headline', 'short_description'])\n",
        "\n",
        "# Drop duplicate rows\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Drop rows where the date is from 2012 to 2016\n",
        "df = df[~df['date'].astype(str).str.startswith(('2012', '2013', '2014', '2015', '2016'))]\n",
        "\n",
        "# Display dataset shape after cleaning\n",
        "print(\"\\nDataset Shape After Cleaning:\", df.shape)\n",
        "\n",
        "# Combine relevant text columns\n",
        "df['text'] = df[['headline', 'short_description']].astype(str).apply(lambda x: ' '.join(x), axis=1)"
      ],
      "metadata": {
        "id": "3BbJZ3OE6rRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-Processing"
      ],
      "metadata": {
        "id": "pyi9AoLobgt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^a-zA-Z\\\\s]', '', text)  # Remove punctuation & numbers\n",
        "    text = re.sub(r'\\\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "df['processed_text'] = df['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "xx1WXoez6V4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize, remove stopwords, and apply lemmatization\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def tokenize_and_lemmatize(text):\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(text) if word not in stop_words])\n",
        "\n",
        "df['filtered_text'] = df['processed_text'].apply(tokenize_and_lemmatize)"
      ],
      "metadata": {
        "id": "mm01JxYO61Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', max_df=0.7, min_df=5, ngram_range=(1,2))\n",
        "X = vectorizer.fit_transform(df['filtered_text'])"
      ],
      "metadata": {
        "id": "7yeiGXgL62O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply TruncatedSVD for Dimensionality Reduction\n",
        "svd = TruncatedSVD(n_components=3800, random_state=42)\n",
        "X_svd = svd.fit_transform(X)\n",
        "explained_variance_svd = svd.explained_variance_ratio_.sum()\n",
        "print(f\"Explained Variance (SVD): {explained_variance_svd:.4f}\")"
      ],
      "metadata": {
        "id": "Z18jdLQP63oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply UMAP for Non-Linear Dimensionality Reduction\n",
        "umap_model = umap.UMAP(n_components=2, n_neighbors=30, min_dist=0.3, random_state=42)\n",
        "X_umap = umap_model.fit_transform(X_svd)\n",
        "explained_variance_umap = np.var(X_umap, axis=0).sum()\n",
        "print(f\"Explained Variance (UMAP): {explained_variance_umap:.4f}\")"
      ],
      "metadata": {
        "id": "NzqsO7bz64e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize data\n",
        "normalizer = Normalizer()\n",
        "X_normalized = normalizer.fit_transform(X_umap)"
      ],
      "metadata": {
        "id": "D2zT3cVv67Oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HDBSCAN\n"
      ],
      "metadata": {
        "id": "6rzm-2kEb_zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Applying HDBSCAN -----------------\n",
        "hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=50, min_samples=5, metric='euclidean')\n",
        "hdbscan_labels = hdbscan_clusterer.fit_predict(X_svd)\n",
        "df['cluster_hdbscan'] = hdbscan_labels"
      ],
      "metadata": {
        "id": "GVdU1mM1Auz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plots"
      ],
      "metadata": {
        "id": "nd5vXHoJcSQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Evaluate Clustering Quality -----------------\n",
        "silhouette = silhouette_score(X_svd, hdbscan_labels) if len(set(hdbscan_labels)) > 1 else -1\n",
        "calinski_harabasz = calinski_harabasz_score(X_svd, hdbscan_labels) if len(set(hdbscan_labels)) > 1 else -1\n",
        "precision = precision_score(hdbscan_labels, hdbscan_labels, average='macro') if len(set(hdbscan_labels)) > 1 else -1\n",
        "accuracy = accuracy_score(hdbscan_labels, hdbscan_labels) if len(set(hdbscan_labels)) > 1 else -1\n",
        "similarity = cosine_similarity(X_svd).mean()\n",
        "\n",
        "print(f'Silhouette Score: {silhouette:.4f}')\n",
        "print(f'Calinski-Harabasz Score: {calinski_harabasz:.4f}')\n",
        "print(f'Precision Score: {precision:.4f}')\n",
        "print(f'Accuracy Score: {accuracy:.4f}')\n",
        "print(f'Average Similarity Score: {similarity:.4f}')"
      ],
      "metadata": {
        "id": "HihOOMDg6RGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Cloud"
      ],
      "metadata": {
        "id": "21sGmIS29S1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate word cloud for each cluster\n",
        "for cluster in set(hdbscan_labels):\n",
        "    if cluster == -1:\n",
        "        continue\n",
        "    cluster_texts = ' '.join(df.loc[df['cluster_hdbscan'] == cluster, 'filtered_text'])\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(cluster_texts)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Word Cloud for Cluster {cluster}')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "puVJmjei9SUz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}